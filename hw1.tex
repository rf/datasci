\documentclass[10pt]{amsart}

\usepackage{color}
\usepackage{enumitem}
\usepackage{verbatim}

\setlength{\headsep}{.33in}
\setlength{\topsep}{0in}
\setlength{\topmargin}{-1in}
\setlength{\topskip}{0.4in}    % between header and text
\setlength{\textheight}{11in} % height of main text
\setlength{\textwidth}{7.05in}    % width of text
\setlength{\oddsidemargin}{-0.30in} % odd page left margin
\setlength{\evensidemargin}{-0.30in} % even page left margin
\setlength{\parindent}{0in}   % remove paragraph indenting

\linespread{1.13}

\newcommand{\head}[1]{
   \begin{tabular*}{7.1in}{@{}l@{\extracolsep{\fill}}r}
      Russell Frank & \today \\
   \end{tabular*}
   \begin{center} \LARGE #1 \normalsize \end{center}
   \vskip 0.1in
}

\begin{document}

\head{Data Science HW 1}

\textbf{1a.} \\

\begin{tabular}[ll]
  producer & nominal \\
  release\_to\_review\_time & interval \\
  rating & ordinal \\
  helpfulness & ratio \\
  number\_of\_votes & ratio \\
  length\_of\_review\_text & ratio \\
\end{tabular}

\textbf{1b.} Apple. \\

\textbf{1c.} 52.9681794471 %.\\

\textbf{1d.} 52.9681794471 %.\\

\textbf{1e.} \\

\begin{tabular}[ll]
  min & -537 \\
  Q1 & 75.0 \\
  median & 144 \\
  Q3 & 290.0 \\
  max & 11686 \\
\end{tabular}

%TODO: interquartile ranges

A box and whisker plot is a nice way to show quartiles.\\

\textbf{1f.} \\

%TODO: plot graphic

\textbf{1g.} Yes, it is obviously skewed, looking at the histogram.

['Sony', '1516', '0', '0', '5', '0.96', '25', '26332']
['Asus', '101', '0', '1', '5', '0.945', '128', '22492']
['Lenovo', '231', '0', '0', '3', '0.853', '34', '18275']

\textbf{1h.} 0.25054940649068008 \\

So, there is a correlation between the length of reviews and their helpfullness.
In general, lengthier reviews are more helpful. Shorter reviews don't have
enough information to be helpful. 

\textbf{1i.} \\

%TODO: plot graphic

\textbf{2.} \\
  $w = 0.216, x = 0.064$ \\
  $y = 0.84, z = 0.64$ \\

  We went over the theorems regarding AND and OR constructions for families of
  hash functions in class. \\

\textbf{3a.} \\

  $u = [1, -1, 1]$ \\ 
  $v = [1, -1, 1]$ \\ 
  $w = [-1, 1, 1]$ \\

  Basically, each bit in the sketch is 1 if the dot product between the
  vector in question and the hyperplane is $> 0$ and -1 if the dot product 
  is $< 0$.

\textbf{3b.} \\

Similarity sketch(u) sketch(v): 1.0 \\
Similarity sketch(u) sketch(w): -0.333333333333 \\

\textbf{4a.} \\

The Mahalanobis distance reduces to the Euclidean distance when the covariance
matrix is the identity. This is the case when the covariance is 0 between each
different random variable, and obviously the ones on the diagonal correspond to
the covariance between a random variable and itself, which is 1.

\textbf{4b.} \\

When the covariance matrix is a diagonal matrix, the Mahalanobis distance
reduces to the normalized Euclidean distance. In this case, we have random
variables which have some differing variance; and the normalized Euclidean
distance is exactly what it sounds like: a normalized distance. The inverse
of this diagonal matrix is therefore going to be $\frac{1}{\delta}$, where
$\delta$ is the variance of the corresponding variable. Since it's in the
denominator, it ends up dividing each component of the distance, resulting in
a normalized distance.

\textbf{5a.} \\

Only the last function is a true permutation.

\textbf{5b.} \\

\textbf{5c.} \\

\textbf{6.} \\

\begin{tabular}[l]
  \textbf{A} \\

  % Mr. and Mrs. Dursley of number four, Privet Drive, were proud to say that they 
  % were perfectly normal, thank you very much.

  Mr. \\
  and Mrs. Dursely \\
  of number four\\
  to say \\
  that they were \\

\end{tabular}

\begin{tabular}[l]
  \textbf{B} \\

  % Not for the first time, an argument had broken out over breakfast at number four, 
  % Privet Drive.

  Not \\
  for \\

\end{tabular}

\end{document}
